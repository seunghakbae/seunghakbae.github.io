---
layout: post
title: Data Science Math - 2.2 벡터와 행렬의 연산
tags: [data-science]
---

# 벡터/행렬의 덧셈과 뺄셈
같은 크기를 가진 두 개의 벡터나 행렬은 덧셈과 뺄셈을 할 수 있다. 두 벡터와 행렬에서 같은 위치에 있는 원소끼리 덧셈과 뺄셈을 하면 된다. **요소별(element-wise) 연산** 이라고 한다.

예를 들어, 벡터 x와 y가 있으면,
![](/public/post/2020_04_21_math_dt_2_2/pic1.PNG)

벡터 x와 벡터 y의 덧셈 x + y와 뺄셈 x - y는 다음처럼 계산한다.
![](/public/post/2020_04_21_math_dt_2_2/pic2.PNG)

~~~python
x = np.array([10, 11, 12, 13, 14])
y = np.array([0, 1, 2, 3, 4])

print(x + y)
~~~

~~~
[10 12 14 16 18]
~~~

행렬도 같은 방법으로 뺄셈과 덧셈을 할 수 있다.
~~~python
np.array([[5,6], [7,8]]) + np.array([[10, 20], [30, 40]]) - np.array([[1,2], [3,4]])
~~~

~~~
array([[14, 24],
       [34, 44]])
~~~

&nbsp;
&nbsp;
&nbsp;

# 스칼라와 벡터/행렬의 곱셈
벡터 x또는 행렬 A에 스칼라값 c를 곱하는 것은 **벡터 x 또는 행렬 A의 모든 원소에 스칼라 값 c를 곱하는 것** 과 같다.

![](/public/post/2020_04_21_math_dt_2_2/pic3.PNG)

### 브로드 캐스팅
원래 덧셈과 뺄셈은 크기(차원)가 같은 두 벡터에 대해서만 할 수 있다. 하지만 벡터와 스칼라의 경우 **관레적으로 다음처럼 1-벡터** 를 사용하여 스칼라를 벡터로 변환한 연산을 허용한다. 이를 **브로드캐스팅** 이라고 한다.

![](/public/post/2020_04_21_math_dt_2_2/pic4.PNG)

데이터 분석에서는 원래의 데이터 벡터 x가 아니라 그 데이터 벡터의 각 원소의 평균값을 뺀 **평균제거(mean removed) 벡터** 혹은 **0-평균(zero-mean)벡터** 를 사용하는 경우가 많다.

![](/public/post/2020_04_21_math_dt_2_2/pic5.PNG)

그리고 위 식에서 m은 평균이다.

![](/public/post/2020_04_21_math_dt_2_2/pic6.PNG)

&nbsp;
&nbsp;
&nbsp;

# 선형조합
벡터/행렬에 다음처럼 스칼라값을 곱한 후 더하거나 뺀 것을 백터/행렬의 **선형조합(linear combination)** 이라고 한다.

![](/public/post/2020_04_21_math_dt_2_2/pic7.PNG)

&nbsp;
&nbsp;
&nbsp;

# 벡터와 벡터의 곱셈
벡터를 곱셈하는 방법은 여러가지가 있지만 여기서는 **내적(inner product)** 에 대해서만 다룬다. 벡터 x와 벡터 y의 내적은 다음처럼 표기한다.

![](/public/post/2020_04_21_math_dt_2_2/pic8.PNG)

내적은 다음처럼 점(dot)으로 표기하는 경우도 있어서 **닷 프로덕트(dot product)** 라고도 부르고 <x,y> 기호로 나타낸다.

![](/public/post/2020_04_21_math_dt_2_2/pic9.PNG)

두 벡터를 내적하려면 다음과 같은 조건이 만족되어야 한다.

1. 두 벡터의 차원(길이)이 같아야 한다.
2. 앞의 벡터가 행 벡터이고 뒤의 벡터가 열 벡터여야 한다.

![](/public/post/2020_04_21_math_dt_2_2/pic10.PNG)

두 벡터를 내적한 값은 **스칼라 값** 이 된다.

다음은 두 벡터의 내적의 예이다.

![](/public/post/2020_04_21_math_dt_2_2/pic11.PNG)

넘파이에서는 **dot()** 명령 또는 **@** (at이라고 읽는다)이라는 연산자로 계산한다.

~~~python
x = np.array([[1], [4], [5]])
y = np.array([[4], [5], [6]])

print(x.T @ y)
print(np.dot(x.T, y))
~~~

~~~
[[54]]
[[54]]
~~~

&nbsp;
&nbsp;
&nbsp;

# 벡터의 예시
# 가중합
벡터의 내적은 가중합을 계산할 때 쓰일 수 있다. **가중합(weighted sum)** 이란 복수의 데이터를 단순히 합하는 것이 아니라 각각의 수에 가중치 값을 곱한 후 이 곱셈 결과를 다시 합한 것을 말한다.

만약 데이터 벡터가  x=[x1,⋯,xN]T 이고 가중치 벡터가  w=[w1,⋯,wN]T 이면 데이터 벡터의 가중합은 다음과 같다.

![](/public/post/2020_04_21_math_dt_2_2/pic12.PNG)

이 값을 벡터 x와 w의 곱으로 나타내면   (w^T)x  또는  (x^T)w  라는 간단한 수식으로 표시할 수 있다.

![](/public/post/2020_04_21_math_dt_2_2/pic13.PNG)

예를 들어 쇼핑을 할 때 각 물건의 가격은 데이터 벡터, 각 물건의 수량은 가중치로 생각하여 내적을 구하면 총금액을 계산할 수 있다.

**연습문제**
~~~python
p = np.array([[100], [80], [50]])
n = np.array([[3], [4], [5]])

print(np.dot(p.T, n))
print(p.T @ n)
~~~

~~~
[[870]]
[[870]]
~~~

### 가중평균
가중합의 가중치값을 가중치값의 합으로 나누면 **가중평균(weighted average)** 이 된다.

예를 들어, 일주일에 한 시간만 수업하는 과목은 1학점짜리 과목이고 일주일에 세 시간씩 수업하는 중요한 과목은 3학점짜리 과목이다. 1학점과 3학점 과목의 점수가 각각 100점, 60점이면 학점을 고려한 **가중 평균(weighted average)** 성적은 다음과 같이 계산한다.

![](/public/post/2020_04_21_math_dt_2_2/pic14.PNG)

벡터로 표현된 N개의 데이터의 단순 평균은 다음처럼 생각할 수 있다.

![](/public/post/2020_04_21_math_dt_2_2/pic15.PNG)

&nbsp;
&nbsp;
&nbsp;

### 유사도
**유사도(similarity)는 두 벡터가 닮은 정도를 정량적으로 나타낸 값** 으로 두 벡터가 비슷한 경우에는 유사도가 커지고 비슷하지 않은 경우에는 유사도가 작아진다. 내적을 이용하면 **코사인 유사도(cosine similarity)** 라는 유사도를 계산할 수 있다.

예를 들어, 0과 1을 나타내는 MNIST 이미지에 대한 내적 계산을 해보자.

~~~python
import matplotlib.gridspec as gridspec

digits = load_digits()
d1 = digits.images[0]
d2 = digits.images[10]
d3 = digits.images[1]
d4 = digits.images[11]
v1 = d1.reshape(64, 1)
v2 = d2.reshape(64, 1)
v3 = d3.reshape(64, 1)
v4 = d4.reshape(64, 1)

plt.figure(figsize=(9, 9))
gs = gridspec.GridSpec(1, 8, height_ratios=[1],
                       width_ratios=[9, 1, 9, 1, 9, 1, 9, 1])
for i in range(4):
    plt.subplot(gs[2 * i])
    plt.imshow(eval("d" + str(i + 1)), aspect=1,
               interpolation='nearest', cmap=plt.cm.bone_r)
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])
    plt.title("image {}".format(i + 1))
    plt.subplot(gs[2 * i + 1])
    plt.imshow(eval("v" + str(i + 1)), aspect=0.25,
               interpolation='nearest', cmap=plt.cm.bone_r)
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])
    plt.title("vector {}".format(i + 1))
plt.tight_layout()
plt.show()
~~~

![](/public/post/2020_04_21_math_dt_2_2/pic16.PNG)

"0"이미지와 "0"이미지, 또는 "1"이미지와 "1"이미지의 내적값은 다음과 같다.

~~~python
(v1.T @ v2)[0][0], (v3.T @ v4)[0][0]
~~~

~~~
(3064.0, 3661.0)
~~~
상대적으로 "0"이미지와 "1"이미지, 또는 "1"이미지와 "0"이미지의 내적값은 작다.
~~~python
(v1.T @ v3)[0][0], (v1.T @ v4)[0][0], (v2.T @ v3)[0][0], (v2.T @ v4)[0][0]
~~~

~~~
(1866.0, 1883.0, 2421.0, 2479.0)
~~~

**연습문제**
(1) 내적을 이용한 첫 번째 이미지와 10번째 이미지의 유사도

~~~python
from sklearn.datasets import load_digits

X = load_digits().data

# 첫번째 이미지와 10번째 이미지의 유사도
X[0].T @ X[9]
~~~

~~~
2807.0
~~~

(2) 내적을 이용하여 모든 이미지의 조합에 대해 유사도를 구하라. 어떻게 구현하는 것이 효율적일까?

~~~python
(X @ X.T)
~~~

~~~
array([[3070., 1866., 2264., ..., 2812., 3006., 2898.],
       [1866., 4209., 3432., ..., 3906., 3083., 3307.],
       [2264., 3432., 4388., ..., 4005., 3063., 3697.],
       ...,
       [2812., 3906., 4005., ..., 5092., 3729., 4598.],
       [3006., 3083., 3063., ..., 3729., 4316., 3850.],
       [2898., 3307., 3697., ..., 4598., 3850., 4938.]])
~~~
&nbsp;
&nbsp;
&nbsp;

# 선형회귀 모형(linear regression model)
