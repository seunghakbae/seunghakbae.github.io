---
layout: post
title: Data Science Math - 2.2 벡터와 행렬의 연산
tags: [data-science]
---

# 벡터/행렬의 덧셈과 뺄셈
같은 크기를 가진 두 개의 벡터나 행렬은 덧셈과 뺄셈을 할 수 있다. 두 벡터와 행렬에서 같은 위치에 있는 원소끼리 덧셈과 뺄셈을 하면 된다. **요소별(element-wise) 연산** 이라고 한다.

예를 들어, 벡터 x와 y가 있으면,
![](/public/post/2020_04_21_math_dt_2_2/pic1.PNG)

벡터 x와 벡터 y의 덧셈 x + y와 뺄셈 x - y는 다음처럼 계산한다.
![](/public/post/2020_04_21_math_dt_2_2/pic2.PNG)

~~~python
x = np.array([10, 11, 12, 13, 14])
y = np.array([0, 1, 2, 3, 4])

print(x + y)
~~~

~~~
[10 12 14 16 18]
~~~

행렬도 같은 방법으로 뺄셈과 덧셈을 할 수 있다.
~~~python
np.array([[5,6], [7,8]]) + np.array([[10, 20], [30, 40]]) - np.array([[1,2], [3,4]])
~~~

~~~
array([[14, 24],
       [34, 44]])
~~~

&nbsp;
&nbsp;
&nbsp;

# 스칼라와 벡터/행렬의 곱셈
벡터 x또는 행렬 A에 스칼라값 c를 곱하는 것은 **벡터 x 또는 행렬 A의 모든 원소에 스칼라 값 c를 곱하는 것** 과 같다.

![](/public/post/2020_04_21_math_dt_2_2/pic3.PNG)

### 브로드 캐스팅
원래 덧셈과 뺄셈은 크기(차원)가 같은 두 벡터에 대해서만 할 수 있다. 하지만 벡터와 스칼라의 경우 **관레적으로 다음처럼 1-벡터** 를 사용하여 스칼라를 벡터로 변환한 연산을 허용한다. 이를 **브로드캐스팅** 이라고 한다.

![](/public/post/2020_04_21_math_dt_2_2/pic4.PNG)

데이터 분석에서는 원래의 데이터 벡터 x가 아니라 그 데이터 벡터의 각 원소의 평균값을 뺀 **평균제거(mean removed) 벡터** 혹은 **0-평균(zero-mean)벡터** 를 사용하는 경우가 많다.

![](/public/post/2020_04_21_math_dt_2_2/pic5.PNG)

그리고 위 식에서 m은 평균이다.

![](/public/post/2020_04_21_math_dt_2_2/pic6.PNG)

&nbsp;
&nbsp;
&nbsp;

# 선형조합
벡터/행렬에 다음처럼 스칼라값을 곱한 후 더하거나 뺀 것을 백터/행렬의 **선형조합(linear combination)** 이라고 한다.

![](/public/post/2020_04_21_math_dt_2_2/pic7.PNG)

&nbsp;
&nbsp;
&nbsp;

# 벡터와 벡터의 곱셈
벡터를 곱셈하는 방법은 여러가지가 있지만 여기서는 **내적(inner product)** 에 대해서만 다룬다. 벡터 x와 벡터 y의 내적은 다음처럼 표기한다.

![](/public/post/2020_04_21_math_dt_2_2/pic8.PNG)

내적은 다음처럼 점(dot)으로 표기하는 경우도 있어서 **닷 프로덕트(dot product)** 라고도 부르고 <x,y> 기호로 나타낸다.

![](/public/post/2020_04_21_math_dt_2_2/pic9.PNG)

두 벡터를 내적하려면 다음과 같은 조건이 만족되어야 한다.

1. 두 벡터의 차원(길이)이 같아야 한다.
2. 앞의 벡터가 행 벡터이고 뒤의 벡터가 열 벡터여야 한다.

![](/public/post/2020_04_21_math_dt_2_2/pic10.PNG)

두 벡터를 내적한 값은 **스칼라 값** 이 된다.

다음은 두 벡터의 내적의 예이다.

![](/public/post/2020_04_21_math_dt_2_2/pic11.PNG)

넘파이에서는 **dot()** 명령 또는 **@** (at이라고 읽는다)이라는 연산자로 계산한다.

~~~python
x = np.array([[1], [4], [5]])
y = np.array([[4], [5], [6]])

print(x.T @ y)
print(np.dot(x.T, y))
~~~

~~~
[[54]]
[[54]]
~~~

&nbsp;
&nbsp;
&nbsp;

# 벡터의 예시
# 가중합
벡터의 내적은 가중합을 계산할 때 쓰일 수 있다. **가중합(weighted sum)** 이란 복수의 데이터를 단순히 합하는 것이 아니라 각각의 수에 가중치 값을 곱한 후 이 곱셈 결과를 다시 합한 것을 말한다.

만약 데이터 벡터가  x=[x1,⋯,xN]T 이고 가중치 벡터가  w=[w1,⋯,wN]T 이면 데이터 벡터의 가중합은 다음과 같다.

![](/public/post/2020_04_21_math_dt_2_2/pic12.PNG)

이 값을 벡터 x와 w의 곱으로 나타내면   (w^T)x  또는  (x^T)w  라는 간단한 수식으로 표시할 수 있다.

![](/public/post/2020_04_21_math_dt_2_2/pic13.PNG)

예를 들어 쇼핑을 할 때 각 물건의 가격은 데이터 벡터, 각 물건의 수량은 가중치로 생각하여 내적을 구하면 총금액을 계산할 수 있다.

**연습문제**
~~~python
p = np.array([[100], [80], [50]])
n = np.array([[3], [4], [5]])

print(np.dot(p.T, n))
print(p.T @ n)
~~~

~~~
[[870]]
[[870]]
~~~

### 가중평균
가중합의 가중치값을 가중치값의 합으로 나누면 **가중평균(weighted average)** 이 된다.

예를 들어, 일주일에 한 시간만 수업하는 과목은 1학점짜리 과목이고 일주일에 세 시간씩 수업하는 중요한 과목은 3학점짜리 과목이다. 1학점과 3학점 과목의 점수가 각각 100점, 60점이면 학점을 고려한 **가중 평균(weighted average)** 성적은 다음과 같이 계산한다.

![](/public/post/2020_04_21_math_dt_2_2/pic14.PNG)

벡터로 표현된 N개의 데이터의 단순 평균은 다음처럼 생각할 수 있다.

![](/public/post/2020_04_21_math_dt_2_2/pic15.PNG)

&nbsp;
&nbsp;
&nbsp;

### 유사도
**유사도(similarity)는 두 벡터가 닮은 정도를 정량적으로 나타낸 값** 으로 두 벡터가 비슷한 경우에는 유사도가 커지고 비슷하지 않은 경우에는 유사도가 작아진다. 내적을 이용하면 **코사인 유사도(cosine similarity)** 라는 유사도를 계산할 수 있다.

예를 들어, 0과 1을 나타내는 MNIST 이미지에 대한 내적 계산을 해보자.

~~~python
import matplotlib.gridspec as gridspec

digits = load_digits()
d1 = digits.images[0]
d2 = digits.images[10]
d3 = digits.images[1]
d4 = digits.images[11]
v1 = d1.reshape(64, 1)
v2 = d2.reshape(64, 1)
v3 = d3.reshape(64, 1)
v4 = d4.reshape(64, 1)

plt.figure(figsize=(9, 9))
gs = gridspec.GridSpec(1, 8, height_ratios=[1],
                       width_ratios=[9, 1, 9, 1, 9, 1, 9, 1])
for i in range(4):
    plt.subplot(gs[2 * i])
    plt.imshow(eval("d" + str(i + 1)), aspect=1,
               interpolation='nearest', cmap=plt.cm.bone_r)
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])
    plt.title("image {}".format(i + 1))
    plt.subplot(gs[2 * i + 1])
    plt.imshow(eval("v" + str(i + 1)), aspect=0.25,
               interpolation='nearest', cmap=plt.cm.bone_r)
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])
    plt.title("vector {}".format(i + 1))
plt.tight_layout()
plt.show()
~~~

![](/public/post/2020_04_21_math_dt_2_2/pic16.PNG)

"0"이미지와 "0"이미지, 또는 "1"이미지와 "1"이미지의 내적값은 다음과 같다.

~~~python
(v1.T @ v2)[0][0], (v3.T @ v4)[0][0]
~~~

~~~
(3064.0, 3661.0)
~~~
상대적으로 "0"이미지와 "1"이미지, 또는 "1"이미지와 "0"이미지의 내적값은 작다.
~~~python
(v1.T @ v3)[0][0], (v1.T @ v4)[0][0], (v2.T @ v3)[0][0], (v2.T @ v4)[0][0]
~~~

~~~
(1866.0, 1883.0, 2421.0, 2479.0)
~~~

**연습문제**
(1) 내적을 이용한 첫 번째 이미지와 10번째 이미지의 유사도

~~~python
from sklearn.datasets import load_digits

X = load_digits().data

# 첫번째 이미지와 10번째 이미지의 유사도
X[0].T @ X[9]
~~~

~~~
2807.0
~~~

(2) 내적을 이용하여 모든 이미지의 조합에 대해 유사도를 구하라. 어떻게 구현하는 것이 효율적일까?

~~~python
(X @ X.T)
~~~

~~~
array([[3070., 1866., 2264., ..., 2812., 3006., 2898.],
       [1866., 4209., 3432., ..., 3906., 3083., 3307.],
       [2264., 3432., 4388., ..., 4005., 3063., 3697.],
       ...,
       [2812., 3906., 4005., ..., 5092., 3729., 4598.],
       [3006., 3083., 3063., ..., 3729., 4316., 3850.],
       [2898., 3307., 3697., ..., 4598., 3850., 4938.]])
~~~
&nbsp;
&nbsp;
&nbsp;

# 선형회귀 모형(linear regression model)
**선형회귀 모형(linear regression model)** 이란 독립변수 x에서 종속변수 y를 예측하는 방법의 하나로 독립변수 벡터 x와 가중치 벡터 w와의 가중합으로 y에 대한 예측값 ^y를 계산하는 수식을 말한다.

![](/public/post/2020_04_21_math_dt_2_2/pic17.PNG)

이 수식은 다음처럼 벡터의 내적으로 나타낼 수 있다.

![](/public/post/2020_04_21_math_dt_2_2/pic18.PNG)

&nbsp;
&nbsp;
&nbsp;

# 제곱합

데이터의 분산(variance)이나 표준 편차(standard deviation)등을 구하는 경우에는 각각의 데이터를 제곱한 뒤 이 값을 모두 더한 **제곱합(sum of squares)** 을 계산해야 한다.

![](/public/post/2020_04_21_math_dt_2_2/pic19.PNG)

&nbsp;
&nbsp;
&nbsp;

# 행렬과 행렬의 곱셈
벡터의 곱셈을 정의한 후에는 이를 이용하여 행렬의 곱셈을 정의 할 수 있다. 행렬과 행렬을 곱하면 행렬이 된다.

A행렬과 B행렬을 곱한 결과가 C행렬이 된다고 하자. C의 i번째 행, j번째 열의 원소 c_ij의 값은 A 행렬의 i번째 행 벡터 a_i^T와 B행렬의 j번째 열 벡터 b_j의 곱이다.

![](/public/post/2020_04_21_math_dt_2_2/20.PNG)

인공 신경망은 내부적으로 다음과 같이 여러 개의 선형회귀 모형을 사용한다.

![](/public/post/2020_04_21_math_dt_2_2/pic21.PNG)

그리고 위 그림을 행렬식으로 표현하면 다음과 같다.  
![](/public/post/2020_04_21_math_dt_2_2/pic22.PNG)

&nbsp;
&nbsp;
&nbsp;

# 교환 법칙과 분배 법칙
행렬의 곱셈은 곱하는 행렬의 순서를 바꾸는 교환 법칙이 성립되지 않는다. 그러나 **덧셈에 대한 분배 법칙은 성립한다.**

![](/public/post/2020_04_21_math_dt_2_2/pic23.PNG)

**전치 연산도** 마찬가지로 덧셈/뺄셈에 대해 분배 법칙이 성립한다.

![](/public/post/2020_04_21_math_dt_2_2/pic24.PNG)

전치 연산과 곱셈의 경우에는 분배 법칙이 성립하기는 하지만 전치 연산이 분배되면서 곱셈의 순서가 바뀐다.

![](/public/post/2020_04_21_math_dt_2_2/pic25.PNG)

&nbsp;
&nbsp;
&nbsp;

# 열 벡터의 선형조합
행렬 X와 벡터 w의 곱은 행렬 X를 이루는 열벡터 c1,c2,c3, .., cM을 뒤에 곱해지는 벡터 w의 각 성분 w1, w2, w3, ..wM으로 **선형조합(linear combination)** 을 한 결과 벡터와 같다.

![](/public/post/2020_04_21_math_dt_2_2/pic26.PNG)

벡터의 크기를 직사각형으로 표시하면 다음과 같다.

![](/public/post/2020_04_21_math_dt_2_2/pic28.PNG)

&nbsp;
&nbsp;
&nbsp;

# 여러 개의 벡터에 대한 가중합 동시 계산
벡터 하나의 가중합은  wTx  또는  xTw 로 표시할 수 있다는 것을 배웠다. 그런데 만약 이렇게  w  가중치를 사용한 가중합을 하나의 벡터  x 가 아니라 여러 벡터  x1,⋯,xM  개에 대해서 모두 계산해야 한다면 어떻게 해야 할까? 예를 들어 위와 같이 선형 회귀 모형을 사용하여 여러 데이터  x1,x2,x3,⋯,xN  개의 데이터 모두에 대해 예측값  y1,y2,y3,⋯,yN 을 한꺼번에 계산하고 싶다면 다음처럼 데이터 행렬  X 를 사용하여  y^=Xw 라는 수식으로 간단하게 표시할 수 있다.

![](/public/post/2020_04_21_math_dt_2_2/pic29.PNG)

&nbsp;
&nbsp;
&nbsp;

# 잔차
선형 회귀분석(linear regression)을 한 결과는 가중치 벡터  w 라는 형태로 나타나고 예측치는 이 가중치 벡터를 사용한 독립변수 데이터 레코드 즉, 벡터  xi 의 가중합  wTxi 이 된다고 했다. **예측치와 실젯값(target)  yi 의 차이를 오차(error) 혹은 잔차(residual)  ei** 라고 한다. 이러한 잔찻값을 모든 독립변수 벡터에 대해 구하면 잔차 벡터  e 가 된다.

![](/public/post/2020_04_21_math_dt_2_2/pic30.PNG)

![](/public/post/2020_04_21_math_dt_2_2/pic31.PNG)

### 잔차 제곱합
잔차의 크기는 잔차 벡터의 각 원소를 제곱한 후 더한 **잔차 제곱합(RSS : Residual Sum of Squares)** 을 이용하여 구한다.

![](/public/post/2020_04_21_math_dt_2_2/pic32.PNG)

&nbsp;
&nbsp;
&nbsp;

# 이차형식
이 식에서  XTX 는 정방행렬이 되므로 이 정방행렬을  A 라고 이름 붙이면 마지막 항은  wTAw 와 같은 형태가 된다. 벡터의 이차형식(Quadratic Form)이란 이처럼 어떤 벡터와 정방행렬이 '행벡터 x 정방행렬 x 열벡터'의 형식으로 되어 있는 것을 말한다.

이 수식을 풀면  i=1,…,N,j=1,…,N 에 대해 가능한 모든  i,j  쌍의 조합을 구한 다음  i ,  j 에 해당하는 원소  xi ,  xj 를 가중치  ai,j 와 같이 곱한 값  ai,jxixj 의 총합이 된다.

![](/public/post/2020_04_21_math_dt_2_2/pic33.PNG)
